{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SewZM1VgQqc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import os, json, time\n",
    "from transformers import RobertaTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'DC_data/text/.ipynb_checkpoints/': No such file or directory\n",
      "rm: cannot remove 'DC_data/labels/.ipynb_checkpoints/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r DC_data/text/.ipynb_checkpoints/\n",
    "!rm -r DC_data/labels/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht2fWiRRQqdB"
   },
   "source": [
    "### Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1852, 1852)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len should be 1852\n",
    "len(os.listdir('DC_data/text/')), len(os.listdir(\"DC_data/labels/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hotFKUnEQqdF"
   },
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk('./DC_data/text/'):\n",
    "    idx2text = {}\n",
    "    for filename in filenames:\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "        idx = filename.split('.')[0]\n",
    "        with open(dirpath + filename) as f:\n",
    "            abstract = ''\n",
    "            for sentence in f:\n",
    "                abstract += sentence\n",
    "            abstract = abstract.replace('\\n', ' ')\n",
    "            idx2text[idx] = abstract\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('./DC_data/labels/'):\n",
    "    idx2label = [] \n",
    "    for filename in filenames:\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "        idx = filename.split('.')[0]\n",
    "        with open(dirpath + filename) as f:\n",
    "            labels = f.readline()\n",
    "            for label in labels.split('<'):\n",
    "                if not label or label.isspace():\n",
    "                    continue\n",
    "                key_label = label.split('--')[0].strip()\n",
    "                if key_label == 'NULL':\n",
    "                    continue\n",
    "                idx2label.append((idx, key_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cz7WfqX0RGIq"
   },
   "outputs": [],
   "source": [
    "idx2label = list(set(idx2label))\n",
    "df = pd.DataFrame(idx2label, columns=['idx', 'label'])\n",
    "df['text'] = df['idx'].map(idx2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AEZ8DEJTQqdI"
   },
   "outputs": [],
   "source": [
    "label_vals = list(set(df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "J8XzavWjQqdI"
   },
   "outputs": [],
   "source": [
    "df = df.groupby(['idx', 'text']).agg({'label': lambda x: list(x)}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2fo15UdtQqdJ"
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "df = df.join(pd.DataFrame(mlb.fit_transform(df.pop('label')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "yx6k1ZLVQqdL",
    "outputId": "c2ee1a0a-564d-4751-8641-f69a1608cd90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>Activating invasion and metastasis</th>\n",
       "      <th>Avoiding immune destruction</th>\n",
       "      <th>Cellular energetics</th>\n",
       "      <th>Enabling replicative immortality</th>\n",
       "      <th>Evading growth suppressors</th>\n",
       "      <th>Genomic instability and mutation</th>\n",
       "      <th>Inducing angiogenesis</th>\n",
       "      <th>Resisting cell death</th>\n",
       "      <th>Sustaining proliferative signaling</th>\n",
       "      <th>Tumor promoting inflammation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11724768</td>\n",
       "      <td>Ghrelin was identified in the stomach as an en...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11773160</td>\n",
       "      <td>PURPOSE The epidermal growth factor receptor (...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11774243</td>\n",
       "      <td>Adoptive transfer of immunity against hepatiti...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11781072</td>\n",
       "      <td>The secretion of immunosuppressive factors lik...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11791181</td>\n",
       "      <td>To characterize the impact of increased produc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx  ... Tumor promoting inflammation\n",
       "0  11724768  ...                            0\n",
       "1  11773160  ...                            0\n",
       "2  11774243  ...                            0\n",
       "3  11781072  ...                            0\n",
       "4  11791181  ...                            0\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3-0BgZ4OWADL"
   },
   "outputs": [],
   "source": [
    "del idx2label, idx2text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwpPKvunQqdO"
   },
   "source": [
    "### Model Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder4GB = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/'\n",
    "checkpoint_folder12GB = '/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['bert-base-uncased',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/',\n",
    "'/home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset/Model/Training/Eval loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WvPoUEixQqdQ"
   },
   "outputs": [],
   "source": [
    "class HoCDataset:\n",
    "    def __init__(self, tokenizer, sentences, labels, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        sentence = str(self.sentences[item])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        } \n",
    "\n",
    "    \n",
    "class BERTClass(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(model_path)\n",
    "        self.out = nn.Linear(768, 10)\n",
    "\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        \n",
    "        return self.out(output)\n",
    "    \n",
    "    \n",
    "def loss_fn(outputs, targets):\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "\n",
    "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d['ids']\n",
    "        mask = d['mask']\n",
    "        token_type_ids = d['token_type_ids']\n",
    "        labels = d['label']\n",
    "        \n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if bi % 50 == 0:\n",
    "            print(f'bi={bi}, loss={loss}')\n",
    "\n",
    "\n",
    "def eval_loop_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_labels = []\n",
    "    fin_outputs = []\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            ids = d['ids'].to(device, dtype=torch.long)\n",
    "            mask = d['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = d['token_type_ids'].to(device, dtype=torch.long)\n",
    "            labels = d['label'].to(device, dtype=torch.long)\n",
    "          \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "          \n",
    "            fin_labels.append(labels.cpu().detach().numpy())\n",
    "            fin_outputs.append(torch.sigmoid(outputs).cpu().detach().numpy())\n",
    "\n",
    "    return np.vstack(fin_outputs), np.vstack(fin_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582,
     "referenced_widgets": [
      "ea262495eccb4baa8952188f7d701a09",
      "374e028286ee4550952aed571fa5b8e9",
      "559982fdd7ae46de85df743272a601af",
      "d95a2f96609343439766b026da2bd9a7",
      "9fa7657a24ea458b920a5b64cf3175a8",
      "94c956d096564875bd739d14506c3207",
      "8d7ca34c0c6d4b8aa0f8ead1e2abf980",
      "68e04fbb9b944b17b7261fcaa0d078d9",
      "1b1cdf041bc64bbfa83002e6575e64f4",
      "fb9c439cf78242d0aead16d5748e31ed",
      "707efb730ae74d37af6cdb06f4e6f83d",
      "6d3c9f5db9474316a1c997861883ed8d",
      "3c7c2a7799894b41a9c495844d738fca",
      "e643d967c1374a5183efc621c8af1bc7",
      "848c0b17e2ce4f7a8d543d8cf1b53d60",
      "e81235c9cc32495fb837ee8c7bd62157",
      "f67af416ffd94e29b868bcb468b3ada8",
      "cf5e0d87e0d54b97a96492b93ed6ddbf",
      "0dbbaa858e364899b8bc214238019426",
      "bdb08dcc5d1644ccabb65b65a8ebb477",
      "8f4abe5eea8a42e991980e7759526c7f",
      "9b46c168785b4bc5b8965b23d00d2da2",
      "eb56c783011f4fc4be056f8b1ec5de3c",
      "40806306d9e543c0847796653c662999",
      "f63554e51d7b49d68d4b7d2de13d3669",
      "bc798e7293c54e498c4a6181d45852e8",
      "b80645b7b79a4524aabc8ff2c3828a7e",
      "e168e0c9c6c64e96ac37d8c87c33dde6",
      "00065408e71a43558d41447f8c9b175a",
      "1016fdd68f5442239e0d084bb212b1b8",
      "4020614a69a2426ab67d5d5954bc673b",
      "ce2bed1ff7ca47daa2b287e3dbf1ca8b",
      "2567426ef8d64153b0f880a7f8ee1097",
      "9e95ac1661c94deeb448699e0fd75530",
      "695d06decc5346eb922b9a0b0af85606",
      "1c6bea2cfa3c4ee2a2f7553be5364df8",
      "d912a054b7bf42ed9e750262c2f36b6d",
      "03335500c71c44d7bbc2f41847a2e780",
      "c0923aa6c50a46618fab89f443ecaaed",
      "158fb3b5490b46e88e73d4d0ffa6fcd1",
      "a43fede8245b46abaa1cd5214e255720",
      "46d0504eb1504517a0f0fd7b6861318a",
      "1ab956068125485e8738f784912aa94e",
      "0ad68382c7974158b2a22498f70e8cbf"
     ]
    },
    "id": "XhpFA7Y2QqdT",
    "outputId": "61fd1cd0-7bb0-48d1-a074-cf9d534d5325"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model bert-base-uncased, with tokenizer bert-base-uncased\n",
      "bi=0, loss=0.6724764108657837\n",
      "bi=50, loss=0.353378027677536\n",
      "bi=100, loss=0.3245600163936615\n",
      "bi=150, loss=0.34938469529151917\n",
      "bi=0, loss=0.27509862184524536\n",
      "bi=50, loss=0.24954788386821747\n",
      "bi=100, loss=0.2549605071544647\n",
      "bi=150, loss=0.22175753116607666\n",
      "bi=0, loss=0.199954554438591\n",
      "bi=50, loss=0.2637068033218384\n",
      "bi=100, loss=0.20407043397426605\n",
      "bi=150, loss=0.15232689678668976\n",
      "bi=0, loss=0.1554514765739441\n",
      "bi=50, loss=0.15801814198493958\n",
      "bi=100, loss=0.18348319828510284\n",
      "bi=150, loss=0.18558908998966217\n",
      "F1 Score (Micro) = 0.7865\n",
      "Training run 1 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model bert-base-uncased, with tokenizer bert-base-uncased\n",
      "bi=0, loss=0.742846667766571\n",
      "bi=50, loss=0.3756067752838135\n",
      "bi=100, loss=0.3423362970352173\n",
      "bi=150, loss=0.29819798469543457\n",
      "bi=0, loss=0.31681180000305176\n",
      "bi=50, loss=0.2845384180545807\n",
      "bi=100, loss=0.23170910775661469\n",
      "bi=150, loss=0.23327182233333588\n",
      "bi=0, loss=0.26353582739830017\n",
      "bi=50, loss=0.21036015450954437\n",
      "bi=100, loss=0.18905289471149445\n",
      "bi=150, loss=0.1947965919971466\n",
      "bi=0, loss=0.2277781069278717\n",
      "bi=50, loss=0.134521946310997\n",
      "bi=100, loss=0.1886909306049347\n",
      "bi=150, loss=0.2373591959476471\n",
      "F1 Score (Micro) = 0.7908\n",
      "Training run 2 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model bert-base-uncased, with tokenizer bert-base-uncased\n",
      "bi=0, loss=0.6898431181907654\n",
      "bi=50, loss=0.34510213136672974\n",
      "bi=100, loss=0.3079327642917633\n",
      "bi=150, loss=0.3499595820903778\n",
      "bi=0, loss=0.38737109303474426\n",
      "bi=50, loss=0.2634445130825043\n",
      "bi=100, loss=0.20501609146595\n",
      "bi=150, loss=0.24982045590877533\n",
      "bi=0, loss=0.241261288523674\n",
      "bi=50, loss=0.19174392521381378\n",
      "bi=100, loss=0.1537034511566162\n",
      "bi=150, loss=0.26159271597862244\n",
      "bi=0, loss=0.1489725112915039\n",
      "bi=50, loss=0.18011103570461273\n",
      "bi=100, loss=0.16214384138584137\n",
      "bi=150, loss=0.12636969983577728\n",
      "F1 Score (Micro) = 0.799\n",
      "Training run 3 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model bert-base-uncased, with tokenizer bert-base-uncased\n",
      "bi=0, loss=0.7164406776428223\n",
      "bi=50, loss=0.3994050621986389\n",
      "bi=100, loss=0.4400630593299866\n",
      "bi=150, loss=0.34033241868019104\n",
      "bi=0, loss=0.31789228320121765\n",
      "bi=50, loss=0.25088241696357727\n",
      "bi=100, loss=0.22026896476745605\n",
      "bi=150, loss=0.17850004136562347\n",
      "bi=0, loss=0.20692096650600433\n",
      "bi=50, loss=0.22179822623729706\n",
      "bi=100, loss=0.15399323403835297\n",
      "bi=150, loss=0.11372482776641846\n",
      "bi=0, loss=0.12165441364049911\n",
      "bi=50, loss=0.13775864243507385\n",
      "bi=100, loss=0.19033734500408173\n",
      "bi=150, loss=0.11200977861881256\n",
      "F1 Score (Micro) = 0.7752\n",
      "Training run 4 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model bert-base-uncased, with tokenizer bert-base-uncased\n",
      "bi=0, loss=0.7057167887687683\n",
      "bi=50, loss=0.4719820022583008\n",
      "bi=100, loss=0.3816884458065033\n",
      "bi=150, loss=0.3994331657886505\n",
      "bi=0, loss=0.26953253149986267\n",
      "bi=50, loss=0.23731456696987152\n",
      "bi=100, loss=0.19651588797569275\n",
      "bi=150, loss=0.2509261667728424\n",
      "bi=0, loss=0.25176578760147095\n",
      "bi=50, loss=0.2076692134141922\n",
      "bi=100, loss=0.23554043471813202\n",
      "bi=150, loss=0.1529245227575302\n",
      "bi=0, loss=0.18941903114318848\n",
      "bi=50, loss=0.1406722515821457\n",
      "bi=100, loss=0.2031526118516922\n",
      "bi=150, loss=0.13043224811553955\n",
      "F1 Score (Micro) = 0.7975\n",
      "Training run 5 completed.\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model 4GB-checkpoints, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6475403308868408\n",
      "bi=50, loss=0.3863500654697418\n",
      "bi=100, loss=0.34903427958488464\n",
      "bi=150, loss=0.40721645951271057\n",
      "bi=0, loss=0.23286424577236176\n",
      "bi=50, loss=0.28720328211784363\n",
      "bi=100, loss=0.206537127494812\n",
      "bi=150, loss=0.269610732793808\n",
      "bi=0, loss=0.12717068195343018\n",
      "bi=50, loss=0.16375528275966644\n",
      "bi=100, loss=0.19675613939762115\n",
      "bi=150, loss=0.22009983658790588\n",
      "bi=0, loss=0.15591135621070862\n",
      "bi=50, loss=0.08291535079479218\n",
      "bi=100, loss=0.1641530841588974\n",
      "bi=150, loss=0.13632315397262573\n",
      "F1 Score (Micro) = 0.7828\n",
      "Training run 1 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model 4GB-checkpoints, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7328251004219055\n",
      "bi=50, loss=0.35392090678215027\n",
      "bi=100, loss=0.281903475522995\n",
      "bi=150, loss=0.32248687744140625\n",
      "bi=0, loss=0.39105674624443054\n",
      "bi=50, loss=0.28608402609825134\n",
      "bi=100, loss=0.16948619484901428\n",
      "bi=150, loss=0.2201278656721115\n",
      "bi=0, loss=0.2989765703678131\n",
      "bi=50, loss=0.14223679900169373\n",
      "bi=100, loss=0.22380514442920685\n",
      "bi=150, loss=0.12068145722150803\n",
      "bi=0, loss=0.12679679691791534\n",
      "bi=50, loss=0.15808716416358948\n",
      "bi=100, loss=0.11753208935260773\n",
      "bi=150, loss=0.19385206699371338\n",
      "F1 Score (Micro) = 0.7882\n",
      "Training run 2 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model 4GB-checkpoints, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7388431429862976\n",
      "bi=50, loss=0.3548583984375\n",
      "bi=100, loss=0.3483312726020813\n",
      "bi=150, loss=0.3180050551891327\n",
      "bi=0, loss=0.24704419076442719\n",
      "bi=50, loss=0.2993348240852356\n",
      "bi=100, loss=0.19525419175624847\n",
      "bi=150, loss=0.15325920283794403\n",
      "bi=0, loss=0.12457270920276642\n",
      "bi=50, loss=0.24779100716114044\n",
      "bi=100, loss=0.15236693620681763\n",
      "bi=150, loss=0.11649186909198761\n",
      "bi=0, loss=0.15995261073112488\n",
      "bi=50, loss=0.08002942055463791\n",
      "bi=100, loss=0.11404677480459213\n",
      "bi=150, loss=0.1274476796388626\n",
      "F1 Score (Micro) = 0.798\n",
      "Training run 3 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model 4GB-checkpoints, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6830863356590271\n",
      "bi=50, loss=0.35056519508361816\n",
      "bi=100, loss=0.42754021286964417\n",
      "bi=150, loss=0.2856346368789673\n",
      "bi=0, loss=0.3423571288585663\n",
      "bi=50, loss=0.22683773934841156\n",
      "bi=100, loss=0.2011721432209015\n",
      "bi=150, loss=0.22281388938426971\n",
      "bi=0, loss=0.2793979346752167\n",
      "bi=50, loss=0.10213303565979004\n",
      "bi=100, loss=0.2067401260137558\n",
      "bi=150, loss=0.1332062929868698\n",
      "bi=0, loss=0.13831372559070587\n",
      "bi=50, loss=0.1714884340763092\n",
      "bi=100, loss=0.09119153022766113\n",
      "bi=150, loss=0.12101290374994278\n",
      "F1 Score (Micro) = 0.7765\n",
      "Training run 4 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-0-3531-4GB and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model 4GB-checkpoints, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6452807784080505\n",
      "bi=50, loss=0.37369081377983093\n",
      "bi=100, loss=0.3907293975353241\n",
      "bi=150, loss=0.3372023105621338\n",
      "bi=0, loss=0.30187031626701355\n",
      "bi=50, loss=0.2511384189128876\n",
      "bi=100, loss=0.18395639955997467\n",
      "bi=150, loss=0.23797443509101868\n",
      "bi=0, loss=0.17913039028644562\n",
      "bi=50, loss=0.19619552791118622\n",
      "bi=100, loss=0.1974482536315918\n",
      "bi=150, loss=0.1849907487630844\n",
      "bi=0, loss=0.12227582931518555\n",
      "bi=50, loss=0.09537488222122192\n",
      "bi=100, loss=0.13914911448955536\n",
      "bi=150, loss=0.09756967425346375\n",
      "F1 Score (Micro) = 0.7834\n",
      "Training run 5 completed.\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-18-67089-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6580801010131836\n",
      "bi=50, loss=0.3801601827144623\n",
      "bi=100, loss=0.29418912529945374\n",
      "bi=150, loss=0.27346116304397583\n",
      "bi=0, loss=0.23131294548511505\n",
      "bi=50, loss=0.2072726935148239\n",
      "bi=100, loss=0.24272394180297852\n",
      "bi=150, loss=0.18033166229724884\n",
      "bi=0, loss=0.08887135237455368\n",
      "bi=50, loss=0.12395989894866943\n",
      "bi=100, loss=0.14382410049438477\n",
      "bi=150, loss=0.09307395666837692\n",
      "bi=0, loss=0.10286219418048859\n",
      "bi=50, loss=0.1073198914527893\n",
      "bi=100, loss=0.07345833629369736\n",
      "bi=150, loss=0.07989972829818726\n",
      "F1 Score (Micro) = 0.7995\n",
      "Training run 1 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-18-67089-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.679334282875061\n",
      "bi=50, loss=0.42574092745780945\n",
      "bi=100, loss=0.3327544331550598\n",
      "bi=150, loss=0.22611720860004425\n",
      "bi=0, loss=0.22599342465400696\n",
      "bi=50, loss=0.18549615144729614\n",
      "bi=100, loss=0.3041066825389862\n",
      "bi=150, loss=0.17194245755672455\n",
      "bi=0, loss=0.12344403564929962\n",
      "bi=50, loss=0.11583267897367477\n",
      "bi=100, loss=0.11548750847578049\n",
      "bi=150, loss=0.11022119969129562\n",
      "bi=0, loss=0.10592450946569443\n",
      "bi=50, loss=0.09232433885335922\n",
      "bi=100, loss=0.10344002395868301\n",
      "bi=150, loss=0.06335185468196869\n",
      "F1 Score (Micro) = 0.8329\n",
      "Training run 2 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-18-67089-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7391654849052429\n",
      "bi=50, loss=0.46718356013298035\n",
      "bi=100, loss=0.35074910521507263\n",
      "bi=150, loss=0.32628536224365234\n",
      "bi=0, loss=0.2309623807668686\n",
      "bi=50, loss=0.2179441899061203\n",
      "bi=100, loss=0.21889828145503998\n",
      "bi=150, loss=0.09413931518793106\n",
      "bi=0, loss=0.13854649662971497\n",
      "bi=50, loss=0.1066657081246376\n",
      "bi=100, loss=0.09866989403963089\n",
      "bi=150, loss=0.12869806587696075\n",
      "bi=0, loss=0.10548462718725204\n",
      "bi=50, loss=0.09573977440595627\n",
      "bi=100, loss=0.0706818550825119\n",
      "bi=150, loss=0.10691659897565842\n",
      "F1 Score (Micro) = 0.8074\n",
      "Training run 3 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-18-67089-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7450225949287415\n",
      "bi=50, loss=0.3957406282424927\n",
      "bi=100, loss=0.29631224274635315\n",
      "bi=150, loss=0.19266565144062042\n",
      "bi=0, loss=0.20268426835536957\n",
      "bi=50, loss=0.2102544754743576\n",
      "bi=100, loss=0.1764812022447586\n",
      "bi=150, loss=0.13881857693195343\n",
      "bi=0, loss=0.15165412425994873\n",
      "bi=50, loss=0.08016572147607803\n",
      "bi=100, loss=0.11153297871351242\n",
      "bi=150, loss=0.11680658906698227\n",
      "bi=0, loss=0.14267410337924957\n",
      "bi=50, loss=0.06964776664972305\n",
      "bi=100, loss=0.11485667526721954\n",
      "bi=150, loss=0.11901070922613144\n",
      "F1 Score (Micro) = 0.8141\n",
      "Training run 4 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-18-67089-4GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-18-67089-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6856151819229126\n",
      "bi=50, loss=0.3977987766265869\n",
      "bi=100, loss=0.2747465670108795\n",
      "bi=150, loss=0.2138931304216385\n",
      "bi=0, loss=0.1968950480222702\n",
      "bi=50, loss=0.12442958354949951\n",
      "bi=100, loss=0.18668685853481293\n",
      "bi=150, loss=0.18855403363704681\n",
      "bi=0, loss=0.1831071674823761\n",
      "bi=50, loss=0.08666256815195084\n",
      "bi=100, loss=0.09120483696460724\n",
      "bi=150, loss=0.0999719500541687\n",
      "bi=0, loss=0.16858656704425812\n",
      "bi=50, loss=0.06641478091478348\n",
      "bi=100, loss=0.07169347256422043\n",
      "bi=150, loss=0.0781402587890625\n",
      "F1 Score (Micro) = 0.8029\n",
      "Training run 5 completed.\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-36-130647-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6644598841667175\n",
      "bi=50, loss=0.32772204279899597\n",
      "bi=100, loss=0.2697829306125641\n",
      "bi=150, loss=0.2395290583372116\n",
      "bi=0, loss=0.22830091416835785\n",
      "bi=50, loss=0.20487360656261444\n",
      "bi=100, loss=0.11495130509138107\n",
      "bi=150, loss=0.21421609818935394\n",
      "bi=0, loss=0.13070209324359894\n",
      "bi=50, loss=0.10683053731918335\n",
      "bi=100, loss=0.13754187524318695\n",
      "bi=150, loss=0.08335988968610764\n",
      "bi=0, loss=0.19950167834758759\n",
      "bi=50, loss=0.14756977558135986\n",
      "bi=100, loss=0.07535725831985474\n",
      "bi=150, loss=0.08801168203353882\n",
      "F1 Score (Micro) = 0.8068\n",
      "Training run 1 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-36-130647-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.69464111328125\n",
      "bi=50, loss=0.32442575693130493\n",
      "bi=100, loss=0.31872662901878357\n",
      "bi=150, loss=0.21977739036083221\n",
      "bi=0, loss=0.19819705188274384\n",
      "bi=50, loss=0.20411725342273712\n",
      "bi=100, loss=0.15032696723937988\n",
      "bi=150, loss=0.17038914561271667\n",
      "bi=0, loss=0.1395380049943924\n",
      "bi=50, loss=0.11765823513269424\n",
      "bi=100, loss=0.1295812427997589\n",
      "bi=150, loss=0.0944443866610527\n",
      "bi=0, loss=0.14768584072589874\n",
      "bi=50, loss=0.09708299487829208\n",
      "bi=100, loss=0.0822601467370987\n",
      "bi=150, loss=0.08006216585636139\n",
      "F1 Score (Micro) = 0.8261\n",
      "Training run 2 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-36-130647-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6808632016181946\n",
      "bi=50, loss=0.3668905198574066\n",
      "bi=100, loss=0.2901049554347992\n",
      "bi=150, loss=0.19353531301021576\n",
      "bi=0, loss=0.24145498871803284\n",
      "bi=50, loss=0.15136544406414032\n",
      "bi=100, loss=0.1761583685874939\n",
      "bi=150, loss=0.13122320175170898\n",
      "bi=0, loss=0.16459636390209198\n",
      "bi=50, loss=0.09544195979833603\n",
      "bi=100, loss=0.18507516384124756\n",
      "bi=150, loss=0.08614809811115265\n",
      "bi=0, loss=0.121103934943676\n",
      "bi=50, loss=0.11064853519201279\n",
      "bi=100, loss=0.07246249914169312\n",
      "bi=150, loss=0.0933104157447815\n",
      "F1 Score (Micro) = 0.8107\n",
      "Training run 3 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-36-130647-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7172778248786926\n",
      "bi=50, loss=0.3581073582172394\n",
      "bi=100, loss=0.3812964856624603\n",
      "bi=150, loss=0.21856561303138733\n",
      "bi=0, loss=0.22957733273506165\n",
      "bi=50, loss=0.18229688704013824\n",
      "bi=100, loss=0.12239282578229904\n",
      "bi=150, loss=0.17700636386871338\n",
      "bi=0, loss=0.12795215845108032\n",
      "bi=50, loss=0.10057883709669113\n",
      "bi=100, loss=0.13921599090099335\n",
      "bi=150, loss=0.10144157707691193\n",
      "bi=0, loss=0.08332540094852448\n",
      "bi=50, loss=0.0910121351480484\n",
      "bi=100, loss=0.09249541163444519\n",
      "bi=150, loss=0.11344435065984726\n",
      "F1 Score (Micro) = 0.8088\n",
      "Training run 4 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/4GB-checkpoints/model-trained-36-130647-4GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-36-130647-4GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6914640665054321\n",
      "bi=50, loss=0.3222445547580719\n",
      "bi=100, loss=0.30015096068382263\n",
      "bi=150, loss=0.21345578134059906\n",
      "bi=0, loss=0.23765766620635986\n",
      "bi=50, loss=0.18687045574188232\n",
      "bi=100, loss=0.195726677775383\n",
      "bi=150, loss=0.1387929767370224\n",
      "bi=0, loss=0.127165749669075\n",
      "bi=50, loss=0.12072036415338516\n",
      "bi=100, loss=0.13026274740695953\n",
      "bi=150, loss=0.11467482894659042\n",
      "bi=0, loss=0.063018299639225\n",
      "bi=50, loss=0.11541266739368439\n",
      "bi=100, loss=0.06993378698825836\n",
      "bi=150, loss=0.10224983841180801\n",
      "F1 Score (Micro) = 0.8093\n",
      "Training run 5 completed.\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-10596-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7061797380447388\n",
      "bi=50, loss=0.3742334544658661\n",
      "bi=100, loss=0.3081986904144287\n",
      "bi=150, loss=0.24085357785224915\n",
      "bi=0, loss=0.19716139137744904\n",
      "bi=50, loss=0.2414478361606598\n",
      "bi=100, loss=0.1640900820493698\n",
      "bi=150, loss=0.1042991429567337\n",
      "bi=0, loss=0.13540063798427582\n",
      "bi=50, loss=0.14539411664009094\n",
      "bi=100, loss=0.15744994580745697\n",
      "bi=150, loss=0.12914292514324188\n",
      "bi=0, loss=0.10329421609640121\n",
      "bi=50, loss=0.1719946712255478\n",
      "bi=100, loss=0.2531307637691498\n",
      "bi=150, loss=0.07521646469831467\n",
      "F1 Score (Micro) = 0.8\n",
      "Training run 1 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-10596-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7122626900672913\n",
      "bi=50, loss=0.39190202951431274\n",
      "bi=100, loss=0.3681310713291168\n",
      "bi=150, loss=0.3115191161632538\n",
      "bi=0, loss=0.2042848914861679\n",
      "bi=50, loss=0.20114651322364807\n",
      "bi=100, loss=0.18203632533550262\n",
      "bi=150, loss=0.15924589335918427\n",
      "bi=0, loss=0.19088397920131683\n",
      "bi=50, loss=0.23924219608306885\n",
      "bi=100, loss=0.17918606102466583\n",
      "bi=150, loss=0.18392221629619598\n",
      "bi=0, loss=0.09577661007642746\n",
      "bi=50, loss=0.1088266596198082\n",
      "bi=100, loss=0.20484638214111328\n",
      "bi=150, loss=0.16912634670734406\n",
      "F1 Score (Micro) = 0.836\n",
      "Training run 2 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-10596-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6808533072471619\n",
      "bi=50, loss=0.31389760971069336\n",
      "bi=100, loss=0.3792473375797272\n",
      "bi=150, loss=0.2937498688697815\n",
      "bi=0, loss=0.19910965859889984\n",
      "bi=50, loss=0.21173810958862305\n",
      "bi=100, loss=0.19071748852729797\n",
      "bi=150, loss=0.1516546607017517\n",
      "bi=0, loss=0.17295202612876892\n",
      "bi=50, loss=0.1581106185913086\n",
      "bi=100, loss=0.11363210529088974\n",
      "bi=150, loss=0.16758717596530914\n",
      "bi=0, loss=0.11741330474615097\n",
      "bi=50, loss=0.1110750064253807\n",
      "bi=100, loss=0.10694875568151474\n",
      "bi=150, loss=0.10402894020080566\n",
      "F1 Score (Micro) = 0.8202\n",
      "Training run 3 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-10596-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6770532727241516\n",
      "bi=50, loss=0.39084792137145996\n",
      "bi=100, loss=0.3037228584289551\n",
      "bi=150, loss=0.28678154945373535\n",
      "bi=0, loss=0.20562390983104706\n",
      "bi=50, loss=0.17093342542648315\n",
      "bi=100, loss=0.17019249498844147\n",
      "bi=150, loss=0.11897950619459152\n",
      "bi=0, loss=0.16133569180965424\n",
      "bi=50, loss=0.14636756479740143\n",
      "bi=100, loss=0.13415198028087616\n",
      "bi=150, loss=0.0895083099603653\n",
      "bi=0, loss=0.09942011535167694\n",
      "bi=50, loss=0.1578434854745865\n",
      "bi=100, loss=0.10189606249332428\n",
      "bi=150, loss=0.12221711128950119\n",
      "F1 Score (Micro) = 0.8025\n",
      "Training run 4 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-0-10596-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-0-10596-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6843156218528748\n",
      "bi=50, loss=0.32142025232315063\n",
      "bi=100, loss=0.28824982047080994\n",
      "bi=150, loss=0.22063468396663666\n",
      "bi=0, loss=0.2500593364238739\n",
      "bi=50, loss=0.19985713064670563\n",
      "bi=100, loss=0.202174112200737\n",
      "bi=150, loss=0.16947101056575775\n",
      "bi=0, loss=0.16465330123901367\n",
      "bi=50, loss=0.17999696731567383\n",
      "bi=100, loss=0.07605455070734024\n",
      "bi=150, loss=0.1077437624335289\n",
      "bi=0, loss=0.10680496692657471\n",
      "bi=50, loss=0.08207881450653076\n",
      "bi=100, loss=0.12305495887994766\n",
      "bi=150, loss=0.1506180614233017\n",
      "F1 Score (Micro) = 0.797\n",
      "Training run 5 completed.\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-3-42384-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7273723483085632\n",
      "bi=50, loss=0.30843687057495117\n",
      "bi=100, loss=0.2913966178894043\n",
      "bi=150, loss=0.1480265110731125\n",
      "bi=0, loss=0.2659478187561035\n",
      "bi=50, loss=0.13486815989017487\n",
      "bi=100, loss=0.1435897797346115\n",
      "bi=150, loss=0.10712108761072159\n",
      "bi=0, loss=0.1455584317445755\n",
      "bi=50, loss=0.09461808204650879\n",
      "bi=100, loss=0.11225538700819016\n",
      "bi=150, loss=0.16729365289211273\n",
      "bi=0, loss=0.10773313045501709\n",
      "bi=50, loss=0.08605696260929108\n",
      "bi=100, loss=0.08730166405439377\n",
      "bi=150, loss=0.07700108736753464\n",
      "F1 Score (Micro) = 0.8103\n",
      "Training run 1 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-3-42384-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7245100140571594\n",
      "bi=50, loss=0.3296479880809784\n",
      "bi=100, loss=0.2907155156135559\n",
      "bi=150, loss=0.2657764256000519\n",
      "bi=0, loss=0.13033176958560944\n",
      "bi=50, loss=0.19026094675064087\n",
      "bi=100, loss=0.13766418397426605\n",
      "bi=150, loss=0.16822253167629242\n",
      "bi=0, loss=0.1018982082605362\n",
      "bi=50, loss=0.11608419567346573\n",
      "bi=100, loss=0.07322538644075394\n",
      "bi=150, loss=0.06841284036636353\n",
      "bi=0, loss=0.09844966977834702\n",
      "bi=50, loss=0.10126078128814697\n",
      "bi=100, loss=0.07188476622104645\n",
      "bi=150, loss=0.0899820551276207\n",
      "F1 Score (Micro) = 0.8419\n",
      "Training run 2 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-3-42384-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7183259725570679\n",
      "bi=50, loss=0.35216158628463745\n",
      "bi=100, loss=0.3001633584499359\n",
      "bi=150, loss=0.16576425731182098\n",
      "bi=0, loss=0.21347928047180176\n",
      "bi=50, loss=0.13918708264827728\n",
      "bi=100, loss=0.14648787677288055\n",
      "bi=150, loss=0.13953928649425507\n",
      "bi=0, loss=0.1296585351228714\n",
      "bi=50, loss=0.09915284812450409\n",
      "bi=100, loss=0.12525883316993713\n",
      "bi=150, loss=0.11414437741041183\n",
      "bi=0, loss=0.06828456372022629\n",
      "bi=50, loss=0.07520722597837448\n",
      "bi=100, loss=0.07186811417341232\n",
      "bi=150, loss=0.11122968047857285\n",
      "F1 Score (Micro) = 0.8255\n",
      "Training run 3 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-3-42384-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6989083290100098\n",
      "bi=50, loss=0.3624172806739807\n",
      "bi=100, loss=0.2989831864833832\n",
      "bi=150, loss=0.21388140320777893\n",
      "bi=0, loss=0.1400012969970703\n",
      "bi=50, loss=0.193049356341362\n",
      "bi=100, loss=0.12974515557289124\n",
      "bi=150, loss=0.15236906707286835\n",
      "bi=0, loss=0.10901164263486862\n",
      "bi=50, loss=0.12829245626926422\n",
      "bi=100, loss=0.10790808498859406\n",
      "bi=150, loss=0.07715588063001633\n",
      "bi=0, loss=0.11882102489471436\n",
      "bi=50, loss=0.10065658390522003\n",
      "bi=100, loss=0.13539069890975952\n",
      "bi=150, loss=0.09344418346881866\n",
      "F1 Score (Micro) = 0.7907\n",
      "Training run 4 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-3-42384-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-3-42384-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7073649764060974\n",
      "bi=50, loss=0.370237797498703\n",
      "bi=100, loss=0.2817348539829254\n",
      "bi=150, loss=0.20546288788318634\n",
      "bi=0, loss=0.21508324146270752\n",
      "bi=50, loss=0.22716453671455383\n",
      "bi=100, loss=0.15480725467205048\n",
      "bi=150, loss=0.2486639767885208\n",
      "bi=0, loss=0.10027748346328735\n",
      "bi=50, loss=0.11296536773443222\n",
      "bi=100, loss=0.1007559522986412\n",
      "bi=150, loss=0.1052221804857254\n",
      "bi=0, loss=0.09043741226196289\n",
      "bi=50, loss=0.08403220027685165\n",
      "bi=100, loss=0.054917704313993454\n",
      "bi=150, loss=0.1374337375164032\n",
      "F1 Score (Micro) = 0.8034\n",
      "Training run 5 completed.\n",
      "Logging model stats....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-5-63576-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.691432535648346\n",
      "bi=50, loss=0.41041478514671326\n",
      "bi=100, loss=0.34073305130004883\n",
      "bi=150, loss=0.19473133981227875\n",
      "bi=0, loss=0.16924484074115753\n",
      "bi=50, loss=0.15590298175811768\n",
      "bi=100, loss=0.1601993292570114\n",
      "bi=150, loss=0.1315949708223343\n",
      "bi=0, loss=0.12744036316871643\n",
      "bi=50, loss=0.0779212936758995\n",
      "bi=100, loss=0.1799997240304947\n",
      "bi=150, loss=0.11129485815763474\n",
      "bi=0, loss=0.06659003347158432\n",
      "bi=50, loss=0.06887584179639816\n",
      "bi=100, loss=0.07032113522291183\n",
      "bi=150, loss=0.09467639774084091\n",
      "F1 Score (Micro) = 0.8123\n",
      "Training run 1 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-5-63576-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7406346201896667\n",
      "bi=50, loss=0.34183433651924133\n",
      "bi=100, loss=0.2902926504611969\n",
      "bi=150, loss=0.26816660165786743\n",
      "bi=0, loss=0.17762470245361328\n",
      "bi=50, loss=0.13381117582321167\n",
      "bi=100, loss=0.16872595250606537\n",
      "bi=150, loss=0.18675720691680908\n",
      "bi=0, loss=0.130807027220726\n",
      "bi=50, loss=0.08762097358703613\n",
      "bi=100, loss=0.09050247073173523\n",
      "bi=150, loss=0.16004233062267303\n",
      "bi=0, loss=0.13003619015216827\n",
      "bi=50, loss=0.10754703730344772\n",
      "bi=100, loss=0.07374420762062073\n",
      "bi=150, loss=0.10761453956365585\n",
      "F1 Score (Micro) = 0.8372\n",
      "Training run 2 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-5-63576-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.6771770715713501\n",
      "bi=50, loss=0.39519867300987244\n",
      "bi=100, loss=0.2796936631202698\n",
      "bi=150, loss=0.15073202550411224\n",
      "bi=0, loss=0.16502727568149567\n",
      "bi=50, loss=0.1557181030511856\n",
      "bi=100, loss=0.15564946830272675\n",
      "bi=150, loss=0.1173369437456131\n",
      "bi=0, loss=0.15398333966732025\n",
      "bi=50, loss=0.09786175191402435\n",
      "bi=100, loss=0.14127571880817413\n",
      "bi=150, loss=0.07902289927005768\n",
      "bi=0, loss=0.08364153653383255\n",
      "bi=50, loss=0.0764140784740448\n",
      "bi=100, loss=0.1043485775589943\n",
      "bi=150, loss=0.08634033799171448\n",
      "F1 Score (Micro) = 0.8291\n",
      "Training run 3 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-5-63576-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7335861325263977\n",
      "bi=50, loss=0.3691302239894867\n",
      "bi=100, loss=0.2727179527282715\n",
      "bi=150, loss=0.17970715463161469\n",
      "bi=0, loss=0.17992575466632843\n",
      "bi=50, loss=0.12280433624982834\n",
      "bi=100, loss=0.1305527687072754\n",
      "bi=150, loss=0.13935764133930206\n",
      "bi=0, loss=0.13676123321056366\n",
      "bi=50, loss=0.1763909012079239\n",
      "bi=100, loss=0.11906283348798752\n",
      "bi=150, loss=0.07294964045286179\n",
      "bi=0, loss=0.09469934552907944\n",
      "bi=50, loss=0.1062656044960022\n",
      "bi=100, loss=0.046425338834524155\n",
      "bi=150, loss=0.09574195742607117\n",
      "F1 Score (Micro) = 0.8217\n",
      "Training run 4 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/americanthinker/notebooks/pytorch/NationalSecurityBERT/Modeling/checkpoints/12GB-checkpoints/model-trained-5-63576-12GB/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model model-trained-5-63576-12GB, with tokenizer ../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt\n",
      "bi=0, loss=0.7019864916801453\n",
      "bi=50, loss=0.4045860469341278\n",
      "bi=100, loss=0.26260167360305786\n",
      "bi=150, loss=0.26338788866996765\n",
      "bi=0, loss=0.21676340699195862\n",
      "bi=50, loss=0.2085028886795044\n",
      "bi=100, loss=0.08165328204631805\n",
      "bi=150, loss=0.11604722589254379\n",
      "bi=0, loss=0.11582460254430771\n",
      "bi=50, loss=0.10508110374212265\n",
      "bi=100, loss=0.10520865768194199\n",
      "bi=150, loss=0.0810474082827568\n",
      "bi=0, loss=0.05983774736523628\n",
      "bi=50, loss=0.09452822059392929\n",
      "bi=100, loss=0.08544166386127472\n",
      "bi=150, loss=0.09806782007217407\n",
      "F1 Score (Micro) = 0.823\n",
      "Training run 5 completed.\n",
      "Logging model stats....\n",
      "\n",
      "Total Training/Eval time: 6506.26 seconds\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 3e-5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for model_path in model_paths:\n",
    "    tokenizer_path = ('bert-base-uncased' if model_path == 'bert-base-uncased'\\\n",
    "                  else '../../Preprocessing/Tokenization/wp-vocab-30500-vocab.txt')\n",
    "    model_name = model_path if model_path == 'bert-base-uncased' else model_path.split('/')[-2].split('.')[0]\n",
    "    scores = []\n",
    "    model_stats = {'model_name':model_name,\n",
    "                   'seeds':[],\n",
    "                   'batch_size':TRAIN_BATCH_SIZE,\n",
    "                   'epochs':EPOCHS,\n",
    "                   'metric':'f1-score',\n",
    "                   'scores': [],\n",
    "                   'mean_score':0\n",
    "                    }\n",
    "    for num, seed in enumerate([42,43,44,45,46], 1):\n",
    "        SEED = seed\n",
    "        model_stats['seeds'].append(SEED)\n",
    "        label_cols = list(df.drop(['idx', 'text'], axis=1).columns)\n",
    "        labels = df[label_cols].values\n",
    "\n",
    "        df_train, df_test, labels_train, labels_test = train_test_split(\n",
    "            df, labels, test_size=0.2, random_state=SEED)\n",
    "\n",
    "\n",
    "        tokenizer = transformers.BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        model = BERTClass(model_path)\n",
    "        model = DataParallel(model)\n",
    "        print(f'Using model {model_name}, with tokenizer {tokenizer_path}')\n",
    "        model.to(device)\n",
    "\n",
    "        train_dataset = HoCDataset(\n",
    "            sentences=df_train.text.values,\n",
    "            labels=labels_train,\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=MAX_LEN\n",
    "        )\n",
    "        train_data_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        test_dataset = HoCDataset(\n",
    "            sentences=df_test.text.values,\n",
    "            labels=labels_test,\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=MAX_LEN\n",
    "        )\n",
    "        test_data_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=4,\n",
    "            shuffle=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        num_training_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "            \n",
    "        output, target = eval_loop_fn(test_data_loader, model, device)\n",
    "        preds = np.array(output) >= 0.5\n",
    "        f1_score_micro = metrics.f1_score(target, preds, average='micro')\n",
    "        print(f\"F1 Score (Micro) = {round(f1_score_micro,4)}\")\n",
    "        model_stats['scores'].append(round(f1_score_micro,6))\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(3)\n",
    "        print(f'Training run {num} completed.')\n",
    "    \n",
    "    print('Logging model stats....')\n",
    "    print()\n",
    "    final_score = np.round(np.mean(model_stats['scores']), 4)\n",
    "    model_stats['mean_score'] = final_score\n",
    "    with open('logs/stats.txt', 'a') as f:\n",
    "        f.write(json.dumps(model_stats))\n",
    "        f.write('\\n')\n",
    "\n",
    "end = time.perf_counter() - start\n",
    "print(f'Total Training/Eval time: {round(end, 2)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "finetuing_document_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00065408e71a43558d41447f8c9b175a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "03335500c71c44d7bbc2f41847a2e780": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ad68382c7974158b2a22498f70e8cbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0dbbaa858e364899b8bc214238019426": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1016fdd68f5442239e0d084bb212b1b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "158fb3b5490b46e88e73d4d0ffa6fcd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ab956068125485e8738f784912aa94e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b1cdf041bc64bbfa83002e6575e64f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1c6bea2cfa3c4ee2a2f7553be5364df8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a43fede8245b46abaa1cd5214e255720",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46d0504eb1504517a0f0fd7b6861318a",
      "value": 440473133
     }
    },
    "2567426ef8d64153b0f880a7f8ee1097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "374e028286ee4550952aed571fa5b8e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94c956d096564875bd739d14506c3207",
      "placeholder": "",
      "style": "IPY_MODEL_8d7ca34c0c6d4b8aa0f8ead1e2abf980",
      "value": "Downloading: 100%"
     }
    },
    "3c7c2a7799894b41a9c495844d738fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f67af416ffd94e29b868bcb468b3ada8",
      "placeholder": "",
      "style": "IPY_MODEL_cf5e0d87e0d54b97a96492b93ed6ddbf",
      "value": "Downloading: 100%"
     }
    },
    "4020614a69a2426ab67d5d5954bc673b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "40806306d9e543c0847796653c662999": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e168e0c9c6c64e96ac37d8c87c33dde6",
      "placeholder": "",
      "style": "IPY_MODEL_00065408e71a43558d41447f8c9b175a",
      "value": "Downloading: 100%"
     }
    },
    "46d0504eb1504517a0f0fd7b6861318a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "559982fdd7ae46de85df743272a601af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68e04fbb9b944b17b7261fcaa0d078d9",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b1cdf041bc64bbfa83002e6575e64f4",
      "value": 231508
     }
    },
    "68e04fbb9b944b17b7261fcaa0d078d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "695d06decc5346eb922b9a0b0af85606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0923aa6c50a46618fab89f443ecaaed",
      "placeholder": "",
      "style": "IPY_MODEL_158fb3b5490b46e88e73d4d0ffa6fcd1",
      "value": "Downloading: 100%"
     }
    },
    "6d3c9f5db9474316a1c997861883ed8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c7c2a7799894b41a9c495844d738fca",
       "IPY_MODEL_e643d967c1374a5183efc621c8af1bc7",
       "IPY_MODEL_848c0b17e2ce4f7a8d543d8cf1b53d60"
      ],
      "layout": "IPY_MODEL_e81235c9cc32495fb837ee8c7bd62157"
     }
    },
    "707efb730ae74d37af6cdb06f4e6f83d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "848c0b17e2ce4f7a8d543d8cf1b53d60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f4abe5eea8a42e991980e7759526c7f",
      "placeholder": "",
      "style": "IPY_MODEL_9b46c168785b4bc5b8965b23d00d2da2",
      "value": " 28.0/28.0 [00:00&lt;00:00, 833B/s]"
     }
    },
    "8d7ca34c0c6d4b8aa0f8ead1e2abf980": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f4abe5eea8a42e991980e7759526c7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94c956d096564875bd739d14506c3207": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b46c168785b4bc5b8965b23d00d2da2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e95ac1661c94deeb448699e0fd75530": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_695d06decc5346eb922b9a0b0af85606",
       "IPY_MODEL_1c6bea2cfa3c4ee2a2f7553be5364df8",
       "IPY_MODEL_d912a054b7bf42ed9e750262c2f36b6d"
      ],
      "layout": "IPY_MODEL_03335500c71c44d7bbc2f41847a2e780"
     }
    },
    "9fa7657a24ea458b920a5b64cf3175a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a43fede8245b46abaa1cd5214e255720": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b80645b7b79a4524aabc8ff2c3828a7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc798e7293c54e498c4a6181d45852e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce2bed1ff7ca47daa2b287e3dbf1ca8b",
      "placeholder": "",
      "style": "IPY_MODEL_2567426ef8d64153b0f880a7f8ee1097",
      "value": " 570/570 [00:00&lt;00:00, 17.5kB/s]"
     }
    },
    "bdb08dcc5d1644ccabb65b65a8ebb477": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c0923aa6c50a46618fab89f443ecaaed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce2bed1ff7ca47daa2b287e3dbf1ca8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf5e0d87e0d54b97a96492b93ed6ddbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d912a054b7bf42ed9e750262c2f36b6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ab956068125485e8738f784912aa94e",
      "placeholder": "",
      "style": "IPY_MODEL_0ad68382c7974158b2a22498f70e8cbf",
      "value": " 420M/420M [00:08&lt;00:00, 57.4MB/s]"
     }
    },
    "d95a2f96609343439766b026da2bd9a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb9c439cf78242d0aead16d5748e31ed",
      "placeholder": "",
      "style": "IPY_MODEL_707efb730ae74d37af6cdb06f4e6f83d",
      "value": " 226k/226k [00:00&lt;00:00, 919kB/s]"
     }
    },
    "e168e0c9c6c64e96ac37d8c87c33dde6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e643d967c1374a5183efc621c8af1bc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dbbaa858e364899b8bc214238019426",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bdb08dcc5d1644ccabb65b65a8ebb477",
      "value": 28
     }
    },
    "e81235c9cc32495fb837ee8c7bd62157": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea262495eccb4baa8952188f7d701a09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_374e028286ee4550952aed571fa5b8e9",
       "IPY_MODEL_559982fdd7ae46de85df743272a601af",
       "IPY_MODEL_d95a2f96609343439766b026da2bd9a7"
      ],
      "layout": "IPY_MODEL_9fa7657a24ea458b920a5b64cf3175a8"
     }
    },
    "eb56c783011f4fc4be056f8b1ec5de3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_40806306d9e543c0847796653c662999",
       "IPY_MODEL_f63554e51d7b49d68d4b7d2de13d3669",
       "IPY_MODEL_bc798e7293c54e498c4a6181d45852e8"
      ],
      "layout": "IPY_MODEL_b80645b7b79a4524aabc8ff2c3828a7e"
     }
    },
    "f63554e51d7b49d68d4b7d2de13d3669": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1016fdd68f5442239e0d084bb212b1b8",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4020614a69a2426ab67d5d5954bc673b",
      "value": 570
     }
    },
    "f67af416ffd94e29b868bcb468b3ada8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb9c439cf78242d0aead16d5748e31ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
